{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0b1c32768c3388c46e098f4634bfda4e74ee484e5ffc2ec37cd24883f67512a61",
   "display_name": "Python 3.7.10 64-bit ('tf2_py37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AI 10강 : decision tree, k-nearest neighbor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 인공지능의 원리? : 블랙박스\n",
    "학습 과정을 알 수 없기 때문에 블랙박스라고 불렀다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 결정 트리 Decision tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- 일련의 질문에 대한 결정을 통해 **데이터를 분해**하는 모델로, 설명이 필요할 때 유용하게 사용된다.  \n",
    "- 훈련 데이터에 있는 변수(특성)을 기반으로 새로운 샘플의 클래스 레이블을 추정한다.\n",
    "- 범주형 데이터와 실수형 데이터 모두에 적용할 수 있다.\n",
    "\n",
    "### 과정\n",
    "1. root부터 시작해 **정보이득 information Gain (IG)**이 최대가 되는 특성으로 데이터를 나눈다.\n",
    "2. leaf node가 순수해 질 때 까지 모든 자식노드에서 분할 작업을 반복\n",
    "    - leaf node : 더 이상 자식노드를 가지지 않는 노드\n",
    "    - leaf node가 순수해진다? : 한 노드의 모든 샘플이 같은 클래스에 속할 때 (gini = 0)\n",
    "\n",
    "- 단, 이 작업을 지나치게 하다보면 깊은 트리가 만들어져 **과적합**문제가 발생한다.\n",
    "- $\\therefore$ 트리의 최대 깊이를 제한한다.(가지치기 : pruning)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 목적함수의 목적\n",
    "가장 정보가 풍부한 특성 (IG가 최대) 으로 노드를 나눈다.  \n",
    "$=>$ 트리 알고리즘으로 최적화\n",
    "\n",
    "$$IG(D_{p},f) = I(D_{p}) - \\sum_{j=1}^{m}\\frac{N_{j}}{N_{p}}I(D_{j})$$  \n",
    "\n",
    "- $f$ : 어떠한 분할 기법을 사용할 것인가?\n",
    "- $D_{p}$ : Dataset of Parent node\n",
    "- $D_{j}$ : Dataset of Jth child node\n",
    "- $I$ : Impurity 불순도 지표\n",
    "- $N_{p}$ : 부모 노드에 있는 전체 데이터의 개수 (number)\n",
    "- $N_{j}$ : j번째 자식노드에 있는 전체 데이터의 개수 (number)  \n",
    "\n",
    "$=>$ IG : 부모 노드의 불순도 $I(D_{p})$ 와 자식노드의 불순도 **합**의 $\\sum_{j=1}^{m}\\frac{N_{j}}{N_{p}}I(D_{j})$ **차이**  \n",
    "     $\\therefore$ 자식 노드의 불순도가 낮을 수록 정보이득 IG는 커진다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "이 때, 탐색 공간을 최소화 하기 위해 **이진 트리**구조를 사용하므로 부모노드 $D_{p}$ 는 두 개의 자식노드 $D_{left}$와 $D_{right}$로 나뉜다.  \n",
    "\n",
    "이를 반영하여 식을 재정의하면\n",
    "\n",
    "$$IG(D_{p},f) = I(D_{p}) - \\frac{N_{left}}{N_{p}}I(D_{left}) - \\frac{N_{right}}{N_{p}}I(D_{right})$$  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 불순도 지표  \n",
    "구하기 위한 식이 각각 다르다 \n",
    "\n",
    "1. 지니 불순도 Gini impurity, $I_{G}$\n",
    "    - 엔트로피와 마찬가지로 클래스가 섞인 정도가 클수록 1에 가까워 진다.\n",
    "2. 엔트로피 entropy, $I_{H}$\n",
    "    - 한 노드의 모든 데이터가 같은 클래스라면 **엔트로피는 0**\n",
    "    - 클래스가 균등하게 분포되어있다면 엔트로피는 **최대 1**      \n",
    "    - $\\therefore$ 트리의 상호 의존 정보를 최대화 한다.\n",
    "3. 분류 오차 classification error, $I_{E}$\n",
    "    - 섞일수록 1에 가까워지는것은 다른 지표와 동일하나 **노드의 클래스 확률 변화에 둔감**하므로 권장되지 않는다.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "$I_{G} = 0$ & $I_{H} = 0$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     (40, 40)    \n",
    "#       /  \\      \n",
    "#  (40, 0) (n, m)  "
   ]
  },
  {
   "source": [
    "$I_{G} = 1$ & $I_{H} = 1$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     (40, 40)    \n",
    "#       /  \\      \n",
    "#  (20, 20) (n, m)"
   ]
  },
  {
   "source": [
    "지니 불순도와 엔트로피는 값이 비슷하므로, 잘못 분류될 확률을 최소화 하기 위해 튜닝할 때에는 불순도 지표를 바꾸는 것 보다 가지치기 수준을 바꾸는것이 권장된다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## K - 최근접 이웃 KNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "다른 알고리즘과 근복적으로 다르다.  \n",
    "타 알고리즘은 올바른 매개변수를 찾기 위한 학습과정을 진행하는데 KNN은 학습을 진행하지 않는다.  \n",
    "\n",
    "- 학습을 필요로 하는 알고리즘 \n",
    "    1. 데이터 정제\n",
    "    2. 모델 학습\n",
    "    3. 최종 하이퍼 파라미터 수정\n",
    "    4. 학습 완료 후 학습 데이터 제거 (데이터 저장 공간 불필요)\n",
    "\n",
    "- KNN\n",
    "    - 알고리즘을 실행할 때마다 모든 학습데이터를 통해 분류를 진행\n",
    "    - 실행 시 마다 학습데이터가 필요함\n",
    "    - 고차원 데이터의 계산 복잡도는 훈련 데이터의 수에 비례하여 증가\n",
    "    - 그러나, 데이터 정제가 잘 되어있다면 학습이 필요 없으니 빠르게 결과를 살필 수 있음"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 작동 방식\n",
    "\n",
    "1. 숫자 $k$와 측정기준 선택\n",
    "    - 일반적으로 유클리디안 거리 측정방식 채택\n",
    "2. 분류하려는 데이터에서 $k$개의 최근접 이웃을 찾음\n",
    "3. 다수결 투표를 진행, 투표 결과에 따라 데이터 클래스 레이블 할당\n",
    "    - 투표?\n",
    "\n",
    "### 유의  \n",
    "1. **과적합과 과소적합**이 $k$에 따라 결정되므로, 균형을 잡기 위한 $k$값을 잘 설정해야 한다.\n",
    "2. 차원의 저주? : 고정된 크기의 훈련 데이터셋 차원이 늘어남에 따라 특성 공간이 희소해지는 현상\n",
    "    - 차원의 저주를 피하기 위해 올바른 변수를 선택하고, 차원 축소 기법을 사용해야 한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 결론\n",
    "\n",
    "가장 중요한 것은 데이터의 질이다.\n",
    "\n",
    "### 추가 공부\n",
    "1. 차원 축소\n",
    "2. 데이터 전처리\n",
    "3. 특성 선택"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}