{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0b1c32768c3388c46e098f4634bfda4e74ee484e5ffc2ec37cd24883f67512a61",
   "display_name": "Python 3.7.10 64-bit ('tf2_py37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AI 14강 : Perceptron"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## MCP 맥컬록-피츠 뉴런\n",
    "\n",
    "1943년 워록 맥컬록, 월터 피츠가 발표한 간소화된 뉴런 개념\n",
    "\n",
    "- 뉴런은 뇌의 신경세포와 서로 연결\n",
    "- 화학적 전기신호 처리\n",
    "\n",
    "맥컬록과 피츠는 신경세포를 **이진 출력**을 도출하는 **간단한 논리회로**로 표현"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 퍼셉트론 perceptron\n",
    "\n",
    "1957년 프랭크 로젠블렛 Frank Rosenblatt이 MCP를 기반하여 퍼셉트론 개념 제시\n",
    "\n",
    "- 출력신호를 만들어낼지 말지 여부를 결정하기 위해 입력 특성에 $w$ 가중치를 곱하는 방식\n",
    "- 헵의 학습 규칙 Hebbian learning rule\n",
    "    - 신경망 알고리즘의 근간\n",
    "    - 신경연결주의  \n",
    "        도널드 헵 Donald O. Hebb이 발표한 이론으로, 인간 두뇌의 작용은 개별 신경세포에 의해 결정되는것이 아니라 신경세포**간의 연결 강도**로 결정된다는 내용을 답고 있다. = 신경망\n",
    "    - 근거  \n",
    "        축삭의 끝인 시냅스가 양쪽뉴런에 의해 반복적이고 동시에 활성화된다면 해당 뉴런사이의 연결강도는 강해진다는 관찰 결과\n",
    "- 레이어에는 가중치와 활성화 함수가 숨어있다.\n",
    "    - 가중치 : 신경망모델 학습 시 변하는 값\n",
    "        - '초기화' 작업을 통해 무작위값을 초기에 할당하면, 학습과정에서 특정 값으로 수렴하게 된다.\n",
    "    - 활성화 함수 $f(x)# : 뉴런의 출력값을 결정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4997504446862735\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "tf.compat.v1.set_random_seed(2020)\n",
    "x = 1  # 입력값이 1\n",
    "y = 0  # 원하는 값은 0\n",
    "w = tf.random.normal([1], 0.1)  # 정규분포에서 무작위 값 추출\n",
    "\n",
    "import math\n",
    "def sigmoid(x):  # 활성화함수 중 하나, sigmoid\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "output = sigmoid( x * w )\n",
    "print(output)"
   ]
  },
  {
   "source": [
    "출력값이 원하는 결과값인 0과는 조금 다른데, 이는 가중치가 잘못 설정되어있기 때문이다.  \n",
    "이 때 `y - output` 을 error라고 한다.(기대출력 - 실제출력)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "학습횟수 : 99,\t Error : -0.499998,\t 예측결과 : 0.499998\n",
      "학습횟수 : 199,\t Error : -0.500000,\t 예측결과 : 0.500000\n",
      "학습횟수 : 299,\t Error : -0.500000,\t 예측결과 : 0.500000\n",
      "학습횟수 : 399,\t Error : -0.500000,\t 예측결과 : 0.500000\n",
      "학습횟수 : 499,\t Error : -0.500000,\t 예측결과 : 0.500000\n",
      "학습횟수 : 599,\t Error : -0.500000,\t 예측결과 : 0.500000\n",
      "학습횟수 : 699,\t Error : -0.500000,\t 예측결과 : 0.500000\n",
      "학습횟수 : 799,\t Error : -0.500000,\t 예측결과 : 0.500000\n",
      "학습횟수 : 899,\t Error : -0.500000,\t 예측결과 : 0.500000\n",
      "학습횟수 : 999,\t Error : -0.500000,\t 예측결과 : 0.500000\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    output = sigmoid(x * w)\n",
    "    error = y - output \n",
    "    w = w + w * 0.1 * error  # 경사하강법\n",
    "\n",
    "    if i % 100 == 99:\n",
    "        print(f'학습횟수 : {i},\\t Error : {error:.6f},\\t 예측결과 : {output:.6f}')"
   ]
  },
  {
   "source": [
    "학습시 가중치는 학습률을 이용해 조정한다.\n",
    "\n",
    "$$ w = w + x * \\eta * error$$\n",
    "\n",
    "- $\\eta$, learning rate, 학습률 : 가중치 조정을 위한 하이퍼 파라미터\n",
    "- 보편적으로 $\\eta$는 적당히 작은값으로 설정한다\n",
    "- 가중치 갱신식은 곱셈으로 구성되므로 입력값 $x$에 0이 입력되면 가중치가 더 이상 조정되지 않는다. -> 일정 횟수 이상부터는 학습의 효과가 없다.  \n",
    "\n",
    "$=>$ 이를 해결하기 위한 개념이 **편향**이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**편향**\n",
    "\n",
    "한 쪽으로 치우쳐진 고정값으로, 입력이 0인 경우 아무것도 학습하지 못하는 것을 방지한다.  \n",
    "편향값은 가중치처럼 난수로 초기화된다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "학습횟수 : 99,\t Error : 0.042271,\t 예측결과 : 0.957729\n",
      "학습횟수 : 199,\t Error : 0.030013,\t 예측결과 : 0.969987\n",
      "학습횟수 : 299,\t Error : 0.023219,\t 예측결과 : 0.976781\n",
      "학습횟수 : 399,\t Error : 0.018915,\t 예측결과 : 0.981085\n",
      "학습횟수 : 499,\t Error : 0.015948,\t 예측결과 : 0.984052\n",
      "학습횟수 : 599,\t Error : 0.013781,\t 예측결과 : 0.986219\n",
      "학습횟수 : 699,\t Error : 0.012130,\t 예측결과 : 0.987870\n",
      "학습횟수 : 799,\t Error : 0.010831,\t 예측결과 : 0.989169\n",
      "학습횟수 : 899,\t Error : 0.009782,\t 예측결과 : 0.990218\n",
      "학습횟수 : 999,\t Error : 0.008917,\t 예측결과 : 0.991083\n"
     ]
    }
   ],
   "source": [
    "# 편향값을 포함한 학습 실습\n",
    "\n",
    "x, y = 0, 1\n",
    "w, b = tf.random.normal([1], 0, 1), tf.random.normal([1], 0, 1)\n",
    "\n",
    "for i in range(1000):\n",
    "    output = sigmoid(x * w + 1 * b)\n",
    "    error = y - output\n",
    "    w = w + x * 0.1 * error\n",
    "    b = b + 1 * 0.1 * error\n",
    "\n",
    "    if i % 100 == 99:\n",
    "        print(f'학습횟수 : {i},\\t Error : {error:.6f},\\t 예측결과 : {output:.6f}')"
   ]
  },
  {
   "source": [
    "### 논리연산\n",
    "\n",
    "1. AND 연산과  or 연산은 결과값을 하나로 치환하면 하나의 선으로 참과 거짓을 구분할 수 있다.\n",
    "2. XOR 연산은 두 개의 입력이 서로 다를 때 참을 반환하며 참과 거짓을 하나의 선으로 구분할 수 **없다.**\n",
    "\n",
    "=> 단층 퍼셉트론으로는 XOR 문제를 풀 수 없으나, 다층 퍼셉트론으로는 해결할 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 역전파 알고리즘\n",
    "\n",
    "1986년 'parallel distributed processing' 에서 은닉층을 활용하게 되면 선형 분류 판별선을 여러개 그리는 것과 같은 효과를 얻을 수 있음이 밝혀졌다. 그러나 파라미터가 많아지면서 적절한 가중치와 편향을 학습하는 것이 어렵다는 단점은 여전히 남아 있었는데, 이는 역전파 알고리즘으로 해결하게 된다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}