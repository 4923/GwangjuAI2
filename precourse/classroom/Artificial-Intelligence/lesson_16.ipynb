{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AI 16강 : 강화학습, Reinforcement Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**개요**  \n",
    "강화학습은 에이전트agent가 환경과 상호작용하며 학습하는 알고리즘이다.  \n",
    "환경에는 보상이라는 기준이 있으며, 에이전트는 시행착오를 통해 보상을 최대화 하는 방향을 학습하게 된다.\n",
    "- 에이전트 : 강화학습에서 '의사결정을 하는 역할'을 수행\n",
    "- 패널티 : 음의 보상\n",
    "- 보상 : 양의 보상"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**특징**  \n",
    "1. 시행착오를 통해 학습한다.  \n",
    "2. 비교적 명확한 보상을 설정할 수 있는 문제를 해결할 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> $\\therefore$ 강화학습 : 보상을 최대화하는 의사결정전략! = 순차적인 행동을 알아가는 방법\n",
    "\n",
    "### MDP (Markov Decision Process)\n",
    "\n",
    "- **순차적인 행동**을 알아나가는 방법을 수학적으로 정의한 것\n",
    "- 구성요소\n",
    "    1. 환경 : 에이전트의 의사결정 반영, 반영된 정보를 에이전트에게 전달\n",
    "    2. 상태 : 의사결정의 기반, 의사결정에서 사용되는 관측값, 행동, 보상을 가공\n",
    "        - 행동 : 현재상태에서 취하는 행동 $A_{t}$ 이산적인 행동과 연속적인 행동으로 나뉘며 행동의 종류는 환경에 따라 결정된다.\n",
    "            - 이산적 행동\n",
    "            - 연속적 행동\n",
    "        - 관측 : 환경에서 제공하는 정보\n",
    "            - 시각적 관측\n",
    "            - 수치적 관측\n",
    "    3. 보상함수 : 에이전트가 특정 상태에서 특정 행동을 했을 때 주어지는 보상의 기댓값 $R_{s}^{a}$ 을 정의하는 함수\n",
    "        - $R_{s}^{a} = E[R_{t+1}|S_{t} = s, A = a]$\n",
    "            - 현재 상태에서 현재상태의 행동을 취해서 얻을 수 있는 보상의 값\n",
    "        \n",
    "    4. 상태변환 확률\n",
    "    5. 감가율$\\gamma$ : 강화학습 알고리즘은 시도했던 범위에서만 다른 방법을 시도한다는 단점이 있다. 이 단점을 해결하기 위해 사용하는 척도가 감가율이다. \n",
    "        - 결과 : 0 ~ 1, 1에 가까울 수록 긍정적이다. (미래의 보상에 더 많은 가중치를 둔다.)\n",
    "        - 감가율을 반영한 보상정보는 : 현재 스텝부터 받았던 보상부터 에피소드가 끝날 때 까지 받았던 보상들에 감가율을 스텝 차이만큼 곱하여 더한다. 세상에... 그리고 이 값을 **반환값**이라고 표현한다. **$G_{t}$** ($t$는 에피소드가 종료된 스텝)\n",
    "\n",
    "\n",
    "+ 무작위 움직임과 익숙한 움직임\n",
    "    - 무작위 움직임 : 탐험 Exploration  \n",
    "        알고 있는 길 위주로 학습하는것을 방지하기 위한 방법 2로, 가끔은 무작위로 움직이도록 하는 설정이다.\n",
    "    - 익숙한 움직임 : 이용 활용 - exploitation  \n",
    "        에이전트가 찾아놓은 방법을 위주로 개선방법을 찾는 설정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 의사결정 학습 방법\n",
    "> 어떻게 의사결정을 학습할 수 있을까?\n",
    "\n",
    "학습의 단위인 에피소드가 끝났을 때, 에이전트가 해당 에피소드에서 한 행동정보를 기록한다. 그리고 그 정보를 다음 에피소드에 적용하여 다른 시도를 한다.  \n",
    "다시말해 **현재 스텝에서 받았던 보상으로부터 에피소드가 끝날때 까지 받았던 보상을 더한것을 정보로 이용한다.**\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}