{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0b1c32768c3388c46e098f4634bfda4e74ee484e5ffc2ec37cd24883f67512a61",
   "display_name": "Python 3.7.10 64-bit ('tf2_py37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AI 8강 : 편미분과 최적화문제"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "머신러닝은 데이터의 경향성을 가장 잘 표현하는 함수를 만드는 작업이라고 했다.  \n",
    "함수를 만들기 위해서는 최적화 문제를 해결해야 하고, 이 때 사용하는 기법이 **경사하강법**이다.\n",
    "\n",
    "$$\\hat{y} = \\theta_{0} + \\theta_{1}x$$\n",
    "\n",
    "경사하강법을 쓰기 위해서는 미분을 사용해야 하는데, 머신러닝의 최적화 문제는 **매개변수의 개수만큼** 변수가 있으므로 **편미분**을 사용해야 한다.  \n",
    "이렇게 변수가 여러개인 함수를 **다변수 함수**라고 한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 최적화 문제\n",
    "\n",
    "$$y = \\theta_{0} + \\theta_{1} + \\theta_{2} + \\cdots + \\theta_{n}$$\n",
    "\n",
    "- $\\cdots$ : \\cdots\n",
    "  \n",
    "변수가 하나라면 기울기의 방향으로 매개변수를 이동하면 되지만  \n",
    "변수가 여러개인 다변수 함수는 매개변수마다 기울기와 움직임이 모두 달라지기 때문에 기존의 방법은 사용할 수 없다.  \n",
    "\n",
    "$\\therefore$ 다변수 함수를 미분할때에는 **변수 하나에만 주목하고, 다른 변수는 모두 상수로 취급하는 편미분**을 사용한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 편미분\n",
    "\n",
    "편미분을 하게되면 미분 연산자 $\\frac{d}{d x}$ 의 $d$를 $\\partial$ 연산자로 바꾼다.\n",
    "\n",
    "예를 들어 $h(x_{1}, x_{2}) = x_{1}^{2} + x_{2}^{3}$ 일 때, $h$를 $x_{1}$으로 편미분하면  \n",
    "$\\frac{\\partial}{\\partial x_{1}}h(x_{1}, x_{2}) = 2x_{1}$ 와 같은 결과가 나온다.\n",
    "\n",
    "- [$\\partial$] $del$ : $del$이라고 부르고 LaTex로는 \\partial로 쓴다.\n",
    "- 변수의 값을 **고정** 한다 : 편미분하는 외의 변수를 상수취급 하는 것\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 갱신식 (경사하강법)\n",
    "$$ x := x - \\eta\\frac{d}{dx}g(x) $$\n",
    "\n",
    "x의 최소값을 찾기 위해서는 경사하강법을 사용했다. 이를 갱신식이라고 부른다.  \n",
    "\n",
    "다변수 함수에 경사하강법을 적용하기 위해서는 $g(x)$ 대신 목적함수 $E(X)$을 대입해야하는데 목적함수는 복잡하기 때문에 합성함수의 미분을 사용해야 한다.\n",
    "\n",
    "$$ E(\\theta) = \\frac{1}{2}\\sum_{i=1}^{n} (y^{(i)} - (\\hat{y}^{(i)}) )^{2} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 합성함수의 미분\n",
    "\n",
    "$f(x) = 10 + x^{2}$ 이고  \n",
    "$g(x) = 3 + x$ 라고 할 때,  \n",
    "둘을 합성한 함수는 $f(g(x)) = 10 + (3 + x)^{2}$다.  \n",
    "\n",
    "편의를 위해 각 함수를 $y$와 $u$로 치환한 후 미분하면\n",
    "$f(x) = y$  \n",
    "$g(x) = u$  \n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} * \\frac{du}{dx}$$\n",
    "\n",
    "위처럼 단계적으로 미분할 수 있게 된다.  \n",
    "$\\because$ $f(g(x))$는 $g(x)$에 대한 함수이고, $g(x)$는 $x$에 대한 함수이기 때문"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2*x + 6"
      ],
      "text/latex": "$\\displaystyle 2 x + 6$"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x, y, u = sp.symbols('x y u')\n",
    "\n",
    "u = 3 + x  # g(x) = 3 + x\n",
    "y = 10 + u ** 2  # f(x) = 10 + x ** 2\n",
    "\n",
    "sp.diff(y, x)"
   ]
  },
  {
   "source": [
    "- 경사하강법으 차수가 늘어날 때 규칙이 생기는데, 이를 일반화 한 것을 **다항식 회귀**라고 한다.\n",
    "- 학습 데이터를 과하게 학습시키거나 갱신식의 차수를 키우면 곡선에 가까워져 학습과정에선 더 정확한 결과를 얻을지 몰라도 실제 데이터에서는 오차가 발생한다.이를 **과적합**이라고 한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}