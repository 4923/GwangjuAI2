{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0b1c32768c3388c46e098f4634bfda4e74ee484e5ffc2ec37cd24883f67512a61",
   "display_name": "Python 3.7.10 64-bit ('tf2_py37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AI 15강 : 역전파 Backpropagation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**문제의식**  \n",
    "- 실제 신경망의 손실 함수 식은 다차함수 이므로 정의하거나 미분하기 어렵다.\n",
    "- 신경망의 목적은 '손실함수가 최솟값'일 때의 파라미터를 찾아 올바른 학습 결과를 도출하는 것이며 이이는 회귀분석이나 로지스틱 회귀의 목적과 같다.\n",
    "    - 단, 신경망에서의 학습이 더 많은 파라미터를 요구한다.\n",
    "\n",
    "\n",
    "> 어떻게 효율적인 학습을 할 수 있을까?  \n",
    "> = 어떻게 적절한 파라미터의 수를 구할 수 있을까?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 역전파 backpropagation\n",
    "\n",
    "> 오차를 역방향으로 전파하는 방식\n",
    "\n",
    "1. 영향을 크게 준 값을 더 크게 조정한다.\n",
    "2. 출력값과 지도 데이터 사이에 생기는 오차를 이용해 입력층쪽으로 가중치를 조정한다\n",
    "3. 마찬가지로 경사하강법을 사용한다.\n",
    "    - 손실함수가 최솟값일 때의 가중치로 원래의 가중치를 조정해야 한다.\n",
    "    - 입력값 각각의 손실함수를 고려해야 한다.\n",
    "    - 단, 특정 입력값에서 손실함수 최솟값은 큰 의미가 없다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "** 문제 : 기울기 소멸 문제**  \n",
    "초기에 역전파 알고리즘에서 사용된 활성화 함수는 sigmoid와 softmax였다.\n",
    "- sigmoid : 미분이 최대치가 0.3, 층을 거칠수록 기울기가 0에 수렴하는 문제\n",
    "- softmax : 출력값으로 확률벡터를 얻기위해 사용헸으므로 각 출력 노드의 출력값을 0~1사이의 값이다.\n",
    "\n",
    "합리적 선택을 할 수 있으나 출력값들이 항상 너무 작기 때문에, 신경망이 깊어질수록 출력이 0에 수렴하는 문제가 생겼다. 즉, 오차의 기울기가 줄어들고, 기울기가 결국 소멸되기 때문에 가중치 조정이 이루어지지 않는다.\n",
    "\n",
    "** 해결 **\n",
    "\n",
    "활성화함수 ReLU 제시 - geoffrey hinton\n",
    "**ReLU** : 입력이 음수일때는 0을 출력, 양수일때는 값을 그대로 출력하기 때문에 기울기 소멸 문제가 어느정도 해결된다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}