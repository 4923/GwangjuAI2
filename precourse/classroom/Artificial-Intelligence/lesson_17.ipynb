{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0b1c32768c3388c46e098f4634bfda4e74ee484e5ffc2ec37cd24883f67512a61",
   "display_name": "Python 3.7.10 64-bit ('tf2_py37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AI 17강 : Reinforcement Learning 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "강화학습 : 에이전트가 많은 보상을 받게 하기 위해 최적의 정책을 학습하는 것"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 탐험 Exploration\n",
    "다양한 경험을 할 수 있도록 에이전트의 행동을 결정하는 기법\n",
    "- 무작위 탐색 방법 Random Exploration\n",
    "    - 취할 수 있는 행동 중 임의로 한 가지 방법을 선택한다.\n",
    "    - 너무 많은 방법을 학습하는것도 좋은 방법은 아니다.\n",
    "\n",
    "### 활용\n",
    "학습된 결과에 따라 에이전트의 행동을 결정하는 기법\n",
    "\n",
    "- 탐욕적 방법 Greedy method\n",
    "    - 가장 큰 보상을 줄 것이라고 **기대되는** 행동만을 반복하는 것\n",
    "\n",
    "### 요약  \n",
    "단 한번의 행동에 대해 최대의 보상을 원한다면 활용을,  \n",
    "장기적으로 보상의 총합을 키우고자 한다면 탐험기법을 사용해야 한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 문제\n",
    "1. 탐욕적 방법과 비탐욕적 방법을 적용할 기준이 모호한 상황이 많음  \n",
    "    => 미래에 남아있는 단계에 대한 행동을 선택해야 한다면 : 비탐욕적 행동을 탐험하여 탐욕적 행동을 사용해야 할 상황을 알아내는 것이 좋다.\n",
    "\n",
    "2. 활용과 탐험의 딜레마/갈등 : 하나의 행동을 선택할 때 활용과 탐험을 동시에 할 수 없다.\n",
    "    - 활용 또는 탐험을 선택하는 기준\n",
    "        1. 정밀한 가치 추정값\n",
    "        2. 불확실성\n",
    "        3. 앞으로 남아있는 단계의 개수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 활용과 탐험의 적절한 분배  \n",
    "난제로, 아직 명확한 답이 없다.\n",
    "\n",
    "1. 예) 행동 가치 방법 action-value-method\n",
    "- 행동의 가치를 추정하고 추정값으로부터 행동을 선택하도록 결정\n",
    "- 행동의 가치 : 행동의 선택될 때의 평균보상\n",
    "    - $Q_{t}(a) = \\frac{시각 t 이전에 취해지는 행동 a에 대한 보상의 합}{시각 t 이전에 행동 a를 취하는 횟수}$\n",
    "        - 실제로 받은 보상의 산술평균을 계산하여 참 값을 추정\n",
    "        - 횟수가 0인 경우 0을 대입\n",
    "        - 무한에 가까운 횟수인 경우 큰 수의 법칙에 따라 행동의 실제 가치로 접근한다. ($q_{*}(a) 행동 a의 실제 가치$)\n",
    "\n",
    "2. 입실론 - 탐욕적 방법 $\\epsilon$-greedy\n",
    "- 대부분의 시간동안에는 탐욕적 선택을 수행하고 상대적 빈도수를 작은 값으로 유지하며 **모든 행동을 대상으로 무작위 선택**을 취하는 방법\n",
    "    - 이 때 모든 행동이 선택될 확률은 균등\n",
    "    - 행동 선택은 행동 가치 추정과는 무관\n",
    "- 효용성 미지수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}