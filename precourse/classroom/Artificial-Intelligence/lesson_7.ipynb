{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0b1c32768c3388c46e098f4634bfda4e74ee484e5ffc2ec37cd24883f67512a61",
   "display_name": "Python 3.7.10 64-bit ('tf2_py37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AI 7강 : 경사하강법"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> 머신러닝을 적용한다 $=$ 함수를 만든다\n",
    "\n",
    "### 학습목표\n",
    "\n",
    "1. 상반기 매출을 예측해보자\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "모든 데이터에는 노이즈가 있다. 데이터를 사분면에 놓아두고 가능한 많은 데이터를 지나도록 함수를 만드는것이 머신러닝이다.\n",
    "\n",
    "$$y=ax+b$$\n",
    "사분면에서 일직선을 표현하기 위해서는 기울기 $a$와 절편 $b$을 알아야 하는데 머신러닝에서는 기울기와 절편을 $\\theta$로 표현한다.  \n",
    "    - $[\\theta]$ theta : 통계학에서 미지수나 추정값을 표현하는 방법  \n",
    "위 일차함수를 $\\theta$를 이용해 표현하면 다음과 같아진다.\n",
    " \n",
    "$$\\hat{y} = \\theta_{0} + \\theta_{1}x$$\n",
    "이 때 $\\hat{y}$은 $y$ hat 이라고 읽으며 추정치를 나타낸다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 목적 함수\n",
    "\n",
    "> 머신러닝은 과거의 관측을 기반으로 새로운 샘플의 결과값을 예측한다\n",
    "\n",
    "실제 결과값과 예측 결과값의 차이를 최소화하기 위해 적합한 함수의 parameter, 즉 적합한 `가중치`와 `절편`을 찾는것이 중요하다.  \n",
    "그 차이를 `오차`라고 할 때, `오차`의 합을 최소화하는 함수를 `목적함수`라고 말한다.\n",
    "\n",
    "$$ E(\\theta) = \\frac{1}{2}\\sum_{i=1}^{n} (y^{(i)} - (\\hat{y}^{(i)}) )^{2} $$\n",
    "실제 결과값과 예측 결과값의 차이, 즉 오차 $y^{(i)} - (\\hat{y}^{(i)})$ 를 제곱해서 모두 더하고 $\\sum_{i=1}^{n}$ 반으로 나눈 함수\n",
    "- 오차가 음수일 수 있기 때문에 제곱을 취한다\n",
    "- 미분할 때 결과 값을 간단한 모양으로 만들기 위해 반으로 나눈다.\n",
    "    - 기울기를 변형해도 최소값은 유지되기 때문에 $\\frac{1}{2}$를 곱해도 상관 없다.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "### 최적화 문제\n",
    "\n",
    "목적 함수의 값이 최소가 되는 $\\theta$를 찾는 문제"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 경사하강법 [참고](https://angeloyeo.github.io/2020/08/16/gradient_descent.html)\n",
    "목적 함수의 값을 최소화 시키기 위해 경사를 내려가듯 최소값을 찾는 기법\n",
    "\n",
    "- review : 미분은 특정 구간의 기울기를 구하는 방법이다. 구간을 점점 좁혀 순간의 기울기를 찾을 수 있다.\n",
    "- review 2 : x와 관계없는 값을 미분하면 0이 된다.\n",
    "- review 3 : 도함수의 반대방향으로 이동하면 최소값을 구할 수 있다.\n",
    "\n",
    "$$ x := x - \\eta\\frac{d}{dx}g(x) $$\n",
    "\n",
    "- [$:=$] \\coloneqq : 정의한다\n",
    "- [$\\eta$] \\eta : 학습률, 양의정수를 사용한다\n",
    "    - 학습률에 따라 최솟값에 도달하기 까지 갱신해야 하는 횟수가 달라진다. == 수렴되는 속도가 달라진다\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `python`의 `sympy` 모듈의 `sympy.diff`를 사용해 미분할 수 있다.\n",
    "\n",
    "import sympy as sp\n",
    "from sympy.abc import x  # 라틴-그리스문자 중 x 사용\n",
    "\n",
    "derivative = sp.diff(x**2 + 2*x + 3, x)  # sym.diff(sym.poly(수식), 미분하는 대상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0 - 1.0*x\n"
     ]
    }
   ],
   "source": [
    "from sympy.abc import eta\n",
    "\n",
    "x = 3\n",
    "eta = 0.5\n",
    "\n",
    "result = x - eta * derivative\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'derivative' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5917925f29a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mderivative\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'derivative' is not defined"
     ]
    }
   ],
   "source": [
    "x = 3\n",
    "eta = 0.1\n",
    "\n",
    "result = x - eta * derivative\n",
    "print(result)"
   ]
  },
  {
   "source": [
    "### 결론\n",
    "\n",
    "1. 학습률 ($\\eta$) 의 값에 따라 최소값이 달라지는 것에서 머신러닝의 실험적 성격을 확인할 수 있다.\n",
    "2. 학습률이 너무 크면 보폭이 커지므로 최소값에 접근하기 어렵다.\n",
    "3. 학습률이 너무 작으면 최소값에 도달하기까지 시간이 많이 걸린다.\n",
    "4. $\\therefore$ 학습률에는 정답이 없다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}