{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "> modeling without tensorflow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "\r\n",
    "%13_MathUnits.ipynb"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%13_MathUnits.ipynb` not found.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 순서\r\n",
    "\r\n",
    "1. 데이터 불러오기\r\n",
    "2. 독립변수와 종속변수 분할하기\r\n",
    "3. 파라미터 생성\r\n",
    "    - 어쩔 수 없이 무작위 값\r\n",
    "    - np.zeros\r\n",
    "4. 순전파 연산\r\n",
    "    - mini batch\r\n",
    "5. 역전파 연산\r\n",
    "    - 이번에 추가\r\n",
    "6. 파라미터 갱신\r\n",
    "7. 손실함수 정의\r\n",
    "8. 평가"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Guide\r\n",
    "\r\n",
    "![model guide](img/abalone_mini_without_tf_guide.png)\r\n",
    "\r\n",
    "- main_execute()\r\n",
    "    - tf처럼 hyperparameter를 입력받도록 한다.\r\n",
    "- math utils?\r\n",
    "    - 실행시킬 때 불러올 모듈/데이터들"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# main_execute()\r\n",
    "\r\n",
    "함수의 모든 기능을 `main_execute()` 에서 관리하므로 미리 설계한다. \r\n",
    "\r\n",
    "- 당장은 동작하지 않지만 버튼 먼저 만들자."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def main_execute(epoch_count = 10, mb_size = 2, report = 2, train_ratio = 0.8):\r\n",
    "    load_dataset()\r\n",
    "    weight_initial, bias_initial = init_param()\r\n",
    "    losses_mean_row, accs_mean_row, final_acc = train_and_test(epoch_count, mb_size, report, train_ratio)\r\n",
    "    \r\n",
    "    return weight_initial, bias_initial, losses_mean_row, accs_mean_row, final_acc "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`main_execute()` 에서 만든 버튼\r\n",
    "\r\n",
    "1. `load_dataset()`\r\n",
    "2. `init_param()`\r\n",
    "3. `train_and_test()`\r\n",
    "\r\n",
    "상단의 함수를 생성하자."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MathUnits.ipynb\r\n",
    "\r\n",
    "필요로 하는 파일 (csv 등) 을 ipynb로 작성한다.\r\n",
    "- 별도의 파일로 생성함 : `13_MathUnits.ipynb`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# load_dataset()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def load_dataset():\r\n",
    "    # 데이터 불러오기\r\n",
    "    path = \"dataset/dataset/abalone_mini.csv\"\r\n",
    "    with open(path) as csvfile:\r\n",
    "        csvreader = csv.reader(csvfile)\r\n",
    "        next (csvfile)\r\n",
    "\r\n",
    "        rows = []\r\n",
    "        for row in csvreader:\r\n",
    "            rows.append(row)\r\n",
    "\r\n",
    "    # 자주 쓰는 variable globalization\r\n",
    "    global input_cnt, output_cnt, data\r\n",
    "    \r\n",
    "    # input_cnt, output_cnt 생성\r\n",
    "    input_cnt, output_cnt = 10, 1\r\n",
    "\r\n",
    "    # one hot vector\r\n",
    "    data = np.zeros([len(rows), input_cnt + output_cnt])    # 행은 rows의 개수, 열은 input_cnt + output_cnt\r\n",
    "\r\n",
    "    for idx, row in enumerate(rows):\r\n",
    "        # one hot vector\r\n",
    "        if row[0] == 'M': data[idx, 0] = 0\r\n",
    "        if row[0] == 'F': data[idx, 1] = 1\r\n",
    "        if row[0] == 'I': data[idx, 0] = 2\r\n",
    "\r\n",
    "        # 나머지 복사 \r\n",
    "        data[idx, 3:] = row[1:]\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# load `MathUnits.ipynb`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%13_MathUnits.ipynb"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%13_MathUnits.ipynb` not found.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "load_dataset()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-960ac791287a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-bffd7f5ae4cf>\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"dataset/dataset/abalone_mini.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mcsvreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mnext\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# init_param()\r\n",
    "parameter 초기화\r\n",
    "\r\n",
    "- weight, bias를 무작위 값으로 초기화한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def init_param():\r\n",
    "    global weight, bias \r\n",
    "\r\n",
    "    # weight, bias list\r\n",
    "    weight_initial, bias_initial = [], []\r\n",
    "\r\n",
    "    # set initial value of weight, bias\r\n",
    "    weight  = np.random.normal(RND_MEAN, RND_STD, size = [ input_cnt, output_cnt ])     # 입력값과 행렬곱을 수행하기 위해 input_cnt, output_cnt로 size를 설정\r\n",
    "    bias    = np.zeros([output_cnt]) # np.zeros를 사용한다. (not normal func)\r\n",
    "    print(f\"Initial Weight Value : \\n{weight}\")\r\n",
    "    print(f\"\\nInitial Bias Value : \\n{bias}\")\r\n",
    "\r\n",
    "    # collect previous weight, bias\r\n",
    "    weight_initial.append(weight)\r\n",
    "    bias_initial.append(bias)\r\n",
    "\r\n",
    "    return (weight_initial, bias_initial)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "왜 np.zeros를 사용하나?\r\n",
    "\r\n",
    "- bias의 초기값은 큰 영향을 미치기 때문에 0으로 맞춘다 : 0으로 맞추는 것도 초기화 방식 중 하나\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weight_initial, bias_initial = init_param()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# arrange_data()\r\n",
    "\r\n",
    "- 전체 인덱스 탐색\r\n",
    "- 데이터 뒤섞기\r\n",
    "    - np.random.shuffle(array_name)\r\n",
    "- 학습 및 테스트의 분할 인덱스 탐색\r\n",
    "- 미니배치의 수 탐색 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def arrange_data(mb_size, train_ratio):\r\n",
    "    # 자주 사용하는 데이터 전역변수화\r\n",
    "    global shuffle_map, test_begin_index\r\n",
    "\r\n",
    "    shuffle_map = np.arange(data.shape[0])\r\n",
    "    np.random.shuffle(shuffle_map)     # shuffle_map은 단순히 array를 생성한 값이니 이를 섞어 사용한다.\r\n",
    "\r\n",
    "    mini_batch_step_count = int(data.shape[0] * train_ratio // mb_size)     # 학습 데이터의 개수 // mb_size == 전체 개수의 mini_batch의 개수\r\n",
    "    test_begin_index = mini_batch_step_count * mb_size # 전체 학습 데이터의 개수 == 테스트 데이터화 학습 데이터의 경계\r\n",
    "\r\n",
    "    return mini_batch_step_count    # 상대적으로 잘 쓰지 않으므로 전역변수화가 아닌 return 값으로 설정\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 함수 test \r\n",
    "mini_batch_step_count = arrange_data(mb_size=2, train_ratio=0.8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"mini_batch_step_count : {mini_batch_step_count}\")\r\n",
    "print(f\"shuffle_map : {shuffle_map[:3]}\")   # 돌릴 때 마다 매번 바뀐다.\r\n",
    "print(f\"test_begin_index : {test_begin_index}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# get_test_data()\r\n",
    "\r\n",
    "- 테스트 데이터 분할\r\n",
    "- 독립 및 종속 변수 생성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_test_data():\r\n",
    "    test_data = data[shuffle_map[test_begin_index:]]\r\n",
    "    return test_data[:, :-output_cnt], test_data[:, -output_cnt:]     # 독립변수, 종속변수 분리하여 추출 : test_x, test_y`"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_x, test_y = get_test_data()\r\n",
    "print(test_x)\r\n",
    "print(\"---\" * 30)\r\n",
    "print(test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# get_train_data()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![epoch idea](img/abalone_mini_without_tf_epoch.png)\r\n",
    "\r\n",
    "- epoch이 진행될 때 마다 shuffle 된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "왜 epoch이 진행될 때 마다 shuffle 하는가? : **global minimum**\r\n",
    "\r\n",
    "- loss를 구하기 위해 경사 하강법을 진행하는데, 학습이 진행될 때 마다 경사를 따라 값이 하강한다.\r\n",
    "- 계속 학습 시켰을 때 값이 더 나빠지는 경우가 있다면 학습이 종료 된 것일까? : local minima에 빠졌을 가능성 O\r\n",
    "    - 아직 local minima, global minimum을 한번에 찾는 방식은 없다.\r\n",
    "- 이를 해결하기 위해 학습할 때 마다 데이터를 섞는다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "아래처럼 처음부터 8개를 다 가져오면 안된다.  \r\n",
    "\r\n",
    " $\\because$ mini batch 단위로 학습하므로\r\n",
    "```\r\n",
    "def get_train_data():\r\n",
    "    train_data = data[shuffle_map[:test_begin_index]]      \r\n",
    "````"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\therefore$ minibatch 인덱스의 특징을 찾는다.\r\n",
    "\r\n",
    "minibatch 단위에 따라 인덱스가 달라지는 특징 발견 (0:2, 2:4, 4:6, 6:8)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# [idea]\r\n",
    "# mb_size = 2\r\n",
    "# def get_train_data():\r\n",
    "    # for idx in range((test_begin_index)):\r\n",
    "    #     train_data  = data[shuffle_map[ mb_size * idx : mb_size * (idx+1)]]        # ???????????????? 규칙은 알겠는데 왜 data[shuffle_map~ ]이 됐지?\r\n",
    "    # return (train_data[:, :-output_cnt], train_data[:, -output_cnt:])\r\n",
    "    # ---------------------- \r\n",
    "# 이 때 range는 (==n) 상위 함수에서 받는다 : 따라서 n을 parameter로 받는다. (mb_size도 마찬가지)\r\n",
    "\r\n",
    "def get_train_data(mb_size, n):\r\n",
    "    if n == 0:\r\n",
    "        np.random.shuffle(shuffle_map[: test_begin_index])  # 돌아갈 때 마다 학습데이터 부분만 섞어라.\r\n",
    "    \r\n",
    "    train_data = data[shuffle_map[mb_size*n: mb_size * (n+1)]]\r\n",
    "    return (train_data[:, :-output_cnt], train_data[:, -output_cnt:])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_x, train_y = get_train_data(mb_size = 2, n = 0)\r\n",
    "\r\n",
    "print(train_x)\r\n",
    "print(\"===\" * 30)\r\n",
    "print(train_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# run_train()\r\n",
    "\r\n",
    "- 우선 틀만 생성한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_train(x, y):\r\n",
    "    loss = 0 \r\n",
    "    accuracy = 100\r\n",
    "    return (loss, accuracy)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run_train(0, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# run_test()\r\n",
    "\r\n",
    "- 순전파 연산 : 결과 확인\r\n",
    "- 평가 진행 : 정확도 측정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_test(x,y):\r\n",
    "    accuracy = 96\r\n",
    "    return accuracy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run_test(0,0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train_and_test()\r\n",
    "\r\n",
    "- 학습 및 테스트 기능 실행\r\n",
    "- 정확도 및 손실값 저장\r\n",
    "    - 학습을 얼마나 진행할 것인가? : `main_execute()` 의 `train data ratio` 필요\r\n",
    "-"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_and_test(epoch_count, mb_size, report, train_ratio):\r\n",
    "    mini_batch_step_count = arrange_data(mb_size, train_ratio) \r\n",
    "    text_x, test_y = get_test_data()     # test data에 대한 독립과 종속을 밖으로 반환\r\n",
    "\r\n",
    "    loss_list_mean_row, accuracy_list_mean_row = [], []\r\n",
    "    for epoch in range(epoch_count):\r\n",
    "        # mini batch가 다 돌아야 1 epoch이므로 반복문 하나 더 추가\r\n",
    "\r\n",
    "        loss_list, accuracy_list = [], []\r\n",
    "        for n in range(mini_batch_step_count):\r\n",
    "            # 학습 데이터를 불러온다.\r\n",
    "            train_x, train_y = get_train_data(mb_size, n)      # n : 1epoch을 돌리기 위해 수행하는 mini batch의 step count\r\n",
    "            loss, accuracy = run_train(train_x, train_y)\r\n",
    "\r\n",
    "            # 데이터를 불러올 때에도 minibatch size 만큼 쪼개서 가져오고 있다.\r\n",
    "            # loss와 accuracy를 어떻게 한번에 보여주나? (like tensorflow) : 하나의 epoch에 대한 loss, acc?\r\n",
    "            # -> mini batch들의 값을 모아서 평균을 낸다. : loss_list, accuracy_list\r\n",
    "\r\n",
    "            loss_list.append(loss)\r\n",
    "            accuracy_list.append(accuracy)\r\n",
    "\r\n",
    "            ################################### 0721  \r\n",
    "\r\n",
    "        # epoch이 돌아가는 동안 특정 횟수마다 값을 출력하는 장치 생성 : report\r\n",
    "        if report > 0 and (epoch+1) % report == 0:\r\n",
    "            acc = run_test(test_x, test_y)  # 결과 확인하는 함수, 반환값 : accuracy\r\n",
    "            print(f\"Epoch {epoch+1} : Train - Loss : {np.mean(loss_list):.3f}, Accuracy : {np.mean(accuracy_list):.3f}, Test - Accuracy : {acc:.3f}\")\r\n",
    "            # epoch : 반복문 이기 때문에 0부터 시작한다.\r\n",
    "            # loss, accuracy : loss_list, accuracy_list에 담긴 값을 하나로 만들어야 한다 ==> 평균\r\n",
    "\r\n",
    "        # 시각화를 위한 값 추출 (loss_list와 accuracy_list의 각 평균값)\r\n",
    "        loss_list_mean = np.mean(loss_list)\r\n",
    "        accuracy_list_mean = np.mean(accuracy_list) * 100   # % 변환\r\n",
    "\r\n",
    "        loss_list_mean_row.append(loss_list_mean)   # 반복문 시작되기 전에 리스트 선언\r\n",
    "        accuracy_list_mean_row.append(accuracy_list_mean)   # 마찬가지\r\n",
    "\r\n",
    "    # FINAL TEST: 출력을 위한 구간\r\n",
    "    final_acc = run_test(test_x, test_y)\r\n",
    "    print(\"=\"*30, \"FINAL TEST\", \"=\"*30)\r\n",
    "    print(f\"\\nFinal Test Accuracy : {final_acc:.3f}\")\r\n",
    "\r\n",
    "    return loss_list_mean_row, accuracy_list_mean_row, final_acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss_list_mean_row, accuracy_list_mean_row, final_acc = train_and_test(epoch_count=10,\r\n",
    "                                                                        mb_size=2, \r\n",
    "                                                                        report=1, \r\n",
    "                                                                        train_ratio=0.8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# forward_neuralnet()\r\n",
    "\r\n",
    "- 순전파 연산"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def forward_neuralnet(x):\r\n",
    "    y_hat = np.matmul(x, weight) + bias \r\n",
    "    return y_hat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_hat = forward_neuralnet(train_x)\r\n",
    "print(y_hat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_x.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weight.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# foward_postproc()\r\n",
    "- 손실함수 연산"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def forward_postproc(y_hat, y):\r\n",
    "    diff   = y_hat - y\r\n",
    "    square = np.square(diff)\r\n",
    "    loss   = np.mean(square)\r\n",
    "\r\n",
    "    return loss "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss = forward_postproc(y_hat, train_y)\r\n",
    "print(\"MSE : \", loss)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}