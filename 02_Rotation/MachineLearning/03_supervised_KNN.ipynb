{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## ML overview\n",
    "\n",
    "1. 지도학습은 답이 있는 문제를 대상으로 하는 학습 방식이다.\n",
    "    - 비지도학습은 답이 없는 데이터들의 특성을 파악하는 학습 방식이다.\n",
    "2. 지도학습에는 회귀와 분류가 있다.\n",
    "3. 회귀 regression 는 선형적인 함수 관계를 찾아내는 것이 목표다.\n",
    "    - 예) 보스턴 집 값 예측\n",
    "4. 분류 classification 는 데이터가 분류된 집합 (label) 을 찾아내는 것이 목표다.\n",
    "    - 예) 붓꽃 품종 분류\n",
    "5. 일반화 generalization 는 train dataset으로 학습한 모델이 test set (일반 데이터의 예시) 를 잘 예측하도록 하는 과정이다. ($\\approxeq$ 훈련)\n",
    "6. 과(대)적합 overfitting 은 train dataset를 지나치게 학습하여 발생하는 문제다.\n",
    "    - accuracy : train dataset $\\uparrow$, test dataset $\\downarrow$\n",
    "    - 너무 상세하게 모델링 했을 때에도 발생한다.\n",
    "7. 과소적합 underfitting 은 학습이 충분하지 않을 때 발생한다. \n",
    "    - accuracy : train dataset $\\downarrow$, test dataset $\\downarrow$\n",
    "8. Machine Learning의 목표는 과대적합과 과소적합의 사이에서 가장 일반화가 잘 된 모델을 찾는 것이다.\n",
    "    <details>\n",
    "    <summary>일반화 곡선 (모델 복잡도 곡선)</summary>\n",
    "    <img src=\"https://tensorflowkorea.files.wordpress.com/2017/06/fig2-01.png\" alt=\"모델 복잡도 곡선\" width=\"60%\" />\n",
    "\n",
    "    </details>\n",
    "\n",
    "9. 과적합을 피하기 위한 방법은 아래와 같다. (예시)\n",
    "    1. 훈련데이터의 다양성 보장 (특정 데이터의 개수가 과하게 많거나 적어서는 안 된다.)\n",
    "    2. 9-1이 충족되었을 때 데이터의 양은 많을수록 좋다. (일반적)\n",
    "    3. 규제 Regularization 을 통해 모델 복잡도를 적절하게 설정한다. (너무 복잡하면 과대적합의 가능성이 커지고 너무 간단하면 과소적합의 가능성이 커진다.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# KNN\n",
    "> K-NearestNeighbors model\n",
    "\n",
    "데이터 포인트(알고자 하는 값, 입력값)와 가장 근접한 k개의 이웃을 기준으로 예측값을 결정하는 모델이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### k의 개수에 따른 변화\n",
    "\n",
    "||**k $\\uparrow$ **|**k $\\downarrow$ **|\n",
    "|-|-|-|\n",
    "|모델 복잡도 | $\\uparrow$ | $\\downarrow$ |\n",
    "|k의 개수| 최대 N개 | 최소 1개 |\n",
    "|결정경계| 부드러운 | 거친 |\n",
    "|이상치| 영향 $\\downarrow$ | 영향 $\\uparrow$|\n",
    "- 이웃의 개수가 작아질 수록 부분집합의 개수가 많아지므로 모델 복잡도가 커진다\n",
    "- 분류KNN (kNN classification) 의 이진분류에서 k의 개수는 홀수개여야 한다. 짝수개로 만들면 첫번째 분류로 답이 결정되기 때문이다.\n",
    "\n",
    "<br>\n",
    "\n",
    "||예측 방법|\n",
    "|-|-|\n",
    "|분류| 다수결로 결정|\n",
    "|회귀| k개의 평균으로 결정|\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### params\n",
    "\n",
    "- n_neighbors : 이웃의 수 (default : 5)\n",
    "- weights : 예측에 사용된 가중 함수 (uniform, distance) (default : uniform)\n",
    "    - 회귀로 쓸 때만 필요. 분류에서는 많이 필요하지 않다.\n",
    "- algorithm : 가까운 이웃을 계산하는데 사용되는 알고리즘 (auto, ball_tree, kd_tree, brute)\n",
    "- leaf_size : BallTree 또는 KDTree에 전달 된 리프 크기\n",
    "- p : (1 : minkowski_distance, 2: manhattan_distance 및 euclidean_distance)\n",
    "- metric : 트리에 사용하는 거리 메트릭스\n",
    "- metric_params : 메트릭 함수에 대한 추가 키워드 인수\n",
    "- n_jobs : 이웃 검색을 위해 실행할 병렬 작업 수\n",
    "    - CPU -> GPU처럼 병렬처리하기 때문에 속도가 빨라지긴 한다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 주요 매개변수(Hyperparameter)\n",
    "- 거리측정 방법, 이웃의 수, 가중치 함수 \n",
    "\n",
    "scikit-learn의 params name\n",
    "- metric  :  유클리디언 거리 방식\n",
    "- k : 이웃의 수\n",
    "- weight  : 가중치 함수\n",
    "     -  uniform : 가중치를 동등하게 설정.\n",
    "     -  distance :  가중치를 거리에 반비례하도록 설정\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### K-NN 분류 vs K-NN 회귀\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}